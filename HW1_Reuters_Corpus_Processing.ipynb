{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f78e56f-8974-48f9-b4b6-0ae46413bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: gensim in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: click in /usr/local/opt/python-click/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/Cellar/jupyterlab/4.0.10/libexec/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/Cellar/jupyterlab/4.0.10/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk scikit-learn gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89579dbf-38cc-42a7-937e-9e4858f1f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/jj/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e9611-adba-4374-8ad8-8f9b005491bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9d8472c-30a9-4570-b8f9-cbbb6dca107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text_file, rare_count):\n",
    "    lowered_text = text_file.lower()\n",
    "    tokens = word_tokenize(lowered_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w.strip() for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    tokens_df = pd.DataFrame(tokens, columns =['words'])\n",
    "    tokens_uniques_df = (\n",
    "        tokens_df['words']\n",
    "        .value_counts()\n",
    "        .reset_index(name='count')\n",
    "        .rename(columns={'index': 'words'})\n",
    "    )\n",
    "    tokens_uniques_df = tokens_uniques_df[tokens_uniques_df['count'] > rare_count]\n",
    "    tokens_uniques_df = tokens_uniques_df.sort_values(by='count', ascending=False)\n",
    "    token_dict = dict(zip(tokens_uniques_df[\"words\"], tokens_uniques_df[\"count\"]))\n",
    "    return token_dict\n",
    "\n",
    "def get_top_ten(topic, print_it=False, print_probs=False):\n",
    "    topic_count_sorted = sorted(vocab_topic_probs[topic].items(), key=lambda x: x[1], reverse=True)\n",
    "    if print_it:\n",
    "        if print_probs:\n",
    "            print(topic_count_sorted[:10])\n",
    "        else:\n",
    "            first_terms = [t[0] for t in topic_count_sorted[:10]]\n",
    "            print(first_terms)\n",
    "    return topic_count_sorted[:10]\n",
    "\n",
    "def get_top_topics():\n",
    "    top_ten = {}\n",
    "    for topic in reuters.categories():\n",
    "        top_ten[topic] = {}\n",
    "        top_probs = get_top_ten(topic)\n",
    "        sum_probs_top_ten = sum(count for _, count in top_probs) \n",
    "        top_ten[topic]['probs'] = sum_probs_top_ten\n",
    "        top_ten[topic]['words'] = [w for w, _ in top_probs]\n",
    "    sorted_topics = sorted(top_ten.items(), key=lambda x: x[1]['probs'], reverse=True)\n",
    "    for topic, info in sorted_topics[:10]:\n",
    "        print(f'{topic}: {info[\"words\"]}')\n",
    "    return sorted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d5e3064-e790-44a8-8028-16c7c1b5ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/jj/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396412\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "nltk.download('reuters')\n",
    "from collections import defaultdict\n",
    "\n",
    "topic_docs = defaultdict(list)\n",
    "\n",
    "docs = {}\n",
    "ctr = 0\n",
    "files = [f for f in reuters.fileids() if len(reuters.categories(f)) == 1] # To make it easy let's limit it to single category docs\n",
    "for doc in files:\n",
    "    docs[ctr] = {}\n",
    "    topics = reuters.categories(doc)\n",
    "    text = reuters.raw(doc)\n",
    "    docs[ctr]['text'] = text \n",
    "    docs[ctr]['topics'] = topics\n",
    "    ctr += 1\n",
    "rare_count = 0\n",
    "tokenized_docs = []\n",
    "for key in docs.keys():\n",
    "    doc_tokens = pre_process(docs[key]['text'], rare_count)\n",
    "    docs[key]['token_dict'] = doc_tokens\n",
    "    tokenized_docs.append(doc_tokens)\n",
    "raw_vocab = []\n",
    "# print(tokenized_docs[0]['words'])\n",
    "for token_dict in tokenized_docs:\n",
    "        raw_vocab.extend(token_dict.keys())\n",
    "# print(raw_vocab)\n",
    "vocab = list(set(raw_vocab))\n",
    "print(len(raw_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8af84ce1-5938-4149-a525-29d0a4280bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reuters.categories())\n",
    "topic_vocab = {}\n",
    "\n",
    "for topic in reuters.categories():\n",
    "    topic_vocab[topic] = {}\n",
    "    for key, item in docs.items():\n",
    "        if topic in docs[key]['topics']:\n",
    "            for word, count in item['token_dict'].items():\n",
    "                if word not in topic_vocab[topic].keys():\n",
    "                    topic_vocab[topic][word] = count\n",
    "                else:\n",
    "                    topic_vocab[topic][word] += count\n",
    "\n",
    "vocab_topic_probs = {}\n",
    "\n",
    "for topic in reuters.categories():\n",
    "    vocab_topic_probs[topic] = {}\n",
    "    total_words_in_topic = sum(topic_vocab[topic].values())\n",
    "    for word in vocab:\n",
    "        count = topic_vocab[topic].get(word, 0)\n",
    "        P_word_topic = (count + 1) / (total_words_in_topic + len(vocab))\n",
    "        vocab_topic_probs[topic][word] = P_word_topic\n",
    "# print(vocab_topic_probs['acq'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ba3bf34-2158-4b9f-8149-c29081334e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary()\n",
    "dictionary.add_documents([[w] for w in vocab])\n",
    "corpus = []\n",
    "for key in docs.keys():\n",
    "    token_dict = docs[key]['token_dict']\n",
    "    bow = [(dictionary.token2id[w], c) for w, c in token_dict.items() if w in dictionary.token2id]\n",
    "    corpus.append(bow)\n",
    "num_topics = 10\n",
    "lda = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=10,        \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a1b3d06-76d9-487a-9d66-695e084546b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['pct', 'year', 'januari', 'februari', 'said', 'rose', 'rise', 'billion', 'decemb', 'increas']\n",
      "Topic 1: ['said', 'compani', 'would', 'offer', 'lt', 'share', 'analyst', 'bid', 'merger', 'dlr']\n",
      "Topic 2: ['mln', 'billion', 'stg', 'year', 'profit', 'tonn', 'sugar', 'trade', 'china', 'dlr']\n",
      "Topic 3: ['said', 'oil', 'price', 'dlr', 'tonn', 'product', 'crude', 'mln', 'barrel', 'ga']\n",
      "Topic 4: ['bank', 'rate', 'billion', 'said', 'pct', 'market', 'dollar', 'mark', 'currenc', 'dlr']\n",
      "Topic 5: ['said', 'trade', 'would', 'countri', 'export', 'japan', 'market', 'meet', 'govern', 'offici']\n",
      "Topic 6: ['dlr', 'said', 'mln', 'compani', 'lt', 'sale', 'earn', 'inc', 'corp', 'year']\n",
      "Topic 7: ['lt', 'said', 'ltd', 'pct', 'compani', 'stake', 'unit', 'corp', 'firm', 'group']\n",
      "Topic 8: ['vs', 'mln', 'ct', 'net', 'loss', 'shr', 'dlr', 'lt', 'profit', 'qtr']\n",
      "Topic 9: ['share', 'said', 'stock', 'lt', 'pct', 'dlr', 'offer', 'compani', 'common', 'inc']\n"
     ]
    }
   ],
   "source": [
    "lda_topics = []\n",
    "for i, topic in lda.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    print(f\"Topic {i}: {[word for word, prob in topic]}\")\n",
    "    lda_topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2df6b730-6714-490e-8bcd-9d86f8b264c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn: ['vs', 'mln', 'ct', 'net', 'dlr', 'shr', 'loss', 'lt', 'profit', 'said']\n",
      "acq: ['said', 'lt', 'share', 'dlr', 'compani', 'mln', 'inc', 'pct', 'offer', 'corp']\n",
      "interest: ['rate', 'pct', 'bank', 'said', 'cut', 'interest', 'market', 'prime', 'day', 'billion']\n",
      "crude: ['oil', 'said', 'price', 'crude', 'dlr', 'barrel', 'mln', 'opec', 'bpd', 'product']\n",
      "trade: ['trade', 'said', 'japan', 'billion', 'would', 'dlr', 'year', 'export', 'import', 'japanes']\n",
      "money-supply: ['billion', 'dlr', 'pct', 'bank', 'mln', 'said', 'money', 'week', 'rose', 'januari']\n",
      "money-fx: ['said', 'bank', 'market', 'currenc', 'rate', 'dollar', 'exchang', 'mln', 'stg', 'pct']\n",
      "coffee: ['coffe', 'said', 'export', 'quota', 'produc', 'ico', 'price', 'brazil', 'meet', 'market']\n",
      "sugar: ['sugar', 'said', 'tonn', 'mln', 'year', 'price', 'trader', 'ec', 'export', 'white']\n",
      "cpi: ['pct', 'price', 'februari', 'said', 'inflat', 'januari', 'rise', 'year', 'consum', 'rose']\n"
     ]
    }
   ],
   "source": [
    "tt= get_top_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c13e6f3-b75d-432e-8535-115bc6aaeb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clunky Method topic 'earn':\n",
      "  LDA topic 0: 10.0% common words\n",
      "  LDA topic 1: 30.0% common words\n",
      "  LDA topic 2: 30.0% common words\n",
      "  LDA topic 3: 30.0% common words\n",
      "  LDA topic 4: 20.0% common words\n",
      "  LDA topic 5: 10.0% common words\n",
      "  LDA topic 6: 40.0% common words\n",
      "  LDA topic 7: 20.0% common words\n",
      "  LDA topic 8: 90.0% common words\n",
      "  LDA topic 9: 30.0% common words\n",
      "\n",
      "Clunky Method topic 'acq':\n",
      "  LDA topic 0: 20.0% common words\n",
      "  LDA topic 1: 60.0% common words\n",
      "  LDA topic 2: 20.0% common words\n",
      "  LDA topic 3: 30.0% common words\n",
      "  LDA topic 4: 30.0% common words\n",
      "  LDA topic 5: 10.0% common words\n",
      "  LDA topic 6: 70.0% common words\n",
      "  LDA topic 7: 50.0% common words\n",
      "  LDA topic 8: 30.0% common words\n",
      "  LDA topic 9: 80.0% common words\n",
      "\n",
      "Clunky Method topic 'interest':\n",
      "  LDA topic 0: 30.0% common words\n",
      "  LDA topic 1: 10.0% common words\n",
      "  LDA topic 2: 10.0% common words\n",
      "  LDA topic 3: 10.0% common words\n",
      "  LDA topic 4: 60.0% common words\n",
      "  LDA topic 5: 20.0% common words\n",
      "  LDA topic 6: 10.0% common words\n",
      "  LDA topic 7: 20.0% common words\n",
      "  LDA topic 8: 0.0% common words\n",
      "  LDA topic 9: 20.0% common words\n",
      "\n",
      "Clunky Method topic 'crude':\n",
      "  LDA topic 0: 10.0% common words\n",
      "  LDA topic 1: 20.0% common words\n",
      "  LDA topic 2: 20.0% common words\n",
      "  LDA topic 3: 80.0% common words\n",
      "  LDA topic 4: 20.0% common words\n",
      "  LDA topic 5: 10.0% common words\n",
      "  LDA topic 6: 30.0% common words\n",
      "  LDA topic 7: 10.0% common words\n",
      "  LDA topic 8: 20.0% common words\n",
      "  LDA topic 9: 20.0% common words\n",
      "\n",
      "Clunky Method topic 'trade':\n",
      "  LDA topic 0: 30.0% common words\n",
      "  LDA topic 1: 30.0% common words\n",
      "  LDA topic 2: 40.0% common words\n",
      "  LDA topic 3: 20.0% common words\n",
      "  LDA topic 4: 30.0% common words\n",
      "  LDA topic 5: 50.0% common words\n",
      "  LDA topic 6: 30.0% common words\n",
      "  LDA topic 7: 10.0% common words\n",
      "  LDA topic 8: 10.0% common words\n",
      "  LDA topic 9: 20.0% common words\n",
      "\n",
      "Clunky Method topic 'money-supply':\n",
      "  LDA topic 0: 50.0% common words\n",
      "  LDA topic 1: 20.0% common words\n",
      "  LDA topic 2: 30.0% common words\n",
      "  LDA topic 3: 30.0% common words\n",
      "  LDA topic 4: 50.0% common words\n",
      "  LDA topic 5: 10.0% common words\n",
      "  LDA topic 6: 30.0% common words\n",
      "  LDA topic 7: 20.0% common words\n",
      "  LDA topic 8: 20.0% common words\n",
      "  LDA topic 9: 30.0% common words\n",
      "\n",
      "Clunky Method topic 'money-fx':\n",
      "  LDA topic 0: 20.0% common words\n",
      "  LDA topic 1: 10.0% common words\n",
      "  LDA topic 2: 20.0% common words\n",
      "  LDA topic 3: 20.0% common words\n",
      "  LDA topic 4: 70.0% common words\n",
      "  LDA topic 5: 20.0% common words\n",
      "  LDA topic 6: 20.0% common words\n",
      "  LDA topic 7: 20.0% common words\n",
      "  LDA topic 8: 10.0% common words\n",
      "  LDA topic 9: 20.0% common words\n",
      "\n",
      "Clunky Method topic 'coffee':\n",
      "  LDA topic 0: 10.0% common words\n",
      "  LDA topic 1: 10.0% common words\n",
      "  LDA topic 2: 0.0% common words\n",
      "  LDA topic 3: 20.0% common words\n",
      "  LDA topic 4: 20.0% common words\n",
      "  LDA topic 5: 40.0% common words\n",
      "  LDA topic 6: 10.0% common words\n",
      "  LDA topic 7: 10.0% common words\n",
      "  LDA topic 8: 0.0% common words\n",
      "  LDA topic 9: 10.0% common words\n",
      "\n",
      "Clunky Method topic 'sugar':\n",
      "  LDA topic 0: 20.0% common words\n",
      "  LDA topic 1: 10.0% common words\n",
      "  LDA topic 2: 40.0% common words\n",
      "  LDA topic 3: 40.0% common words\n",
      "  LDA topic 4: 10.0% common words\n",
      "  LDA topic 5: 20.0% common words\n",
      "  LDA topic 6: 30.0% common words\n",
      "  LDA topic 7: 10.0% common words\n",
      "  LDA topic 8: 10.0% common words\n",
      "  LDA topic 9: 10.0% common words\n",
      "\n",
      "Clunky Method topic 'cpi':\n",
      "  LDA topic 0: 70.0% common words\n",
      "  LDA topic 1: 10.0% common words\n",
      "  LDA topic 2: 10.0% common words\n",
      "  LDA topic 3: 20.0% common words\n",
      "  LDA topic 4: 20.0% common words\n",
      "  LDA topic 5: 10.0% common words\n",
      "  LDA topic 6: 20.0% common words\n",
      "  LDA topic 7: 20.0% common words\n",
      "  LDA topic 8: 0.0% common words\n",
      "  LDA topic 9: 20.0% common words\n"
     ]
    }
   ],
   "source": [
    "matching_topics = []\n",
    "\n",
    "''' ChatGPT wrote this and the next section '''\n",
    "for topic_name, info in tt[:10]:\n",
    "    s_words = set(info['words'])\n",
    "    print(f\"\\nClunky Method topic '{topic_name}':\")\n",
    "    for lda_id, lda_words in lda_top_words.items():\n",
    "        lda_set = set(lda_words)\n",
    "        overlap_pct = len(s_words & lda_set) / len(s_words) * 100\n",
    "        print(f\"  LDA topic {lda_id}: {overlap_pct:.1f}% common words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "343af9ed-ca4a-434b-9b0f-f273e8800125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topic -> Best matching Clunky Method topic (% overlap):\n",
      "\n",
      "LDA topic 0: cpi (70.0% match)\n",
      "LDA topic 1: acq (60.0% match)\n",
      "LDA topic 2: trade (40.0% match)\n",
      "LDA topic 3: crude (80.0% match)\n",
      "LDA topic 4: money-fx (70.0% match)\n",
      "LDA topic 5: trade (50.0% match)\n",
      "LDA topic 6: acq (70.0% match)\n",
      "LDA topic 7: acq (50.0% match)\n",
      "LDA topic 8: earn (90.0% match)\n",
      "LDA topic 9: acq (80.0% match)\n",
      "Overall match: 66.0%\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Prepare top words\n",
    "\n",
    "# LDA top words per topic\n",
    "lda_top_words = {\n",
    "    topic_id: [w for w, _ in words]\n",
    "    for topic_id, words in lda.show_topics(num_topics=10, num_words=10, formatted=False)\n",
    "}\n",
    "\n",
    "# Supervised / summed-topic top words\n",
    "supervised_top_words = {\n",
    "    topic_name: set(info['words'])\n",
    "    for topic_name, info in tt[:10]  # top 10 supervised topics\n",
    "}\n",
    "\n",
    "# 2️⃣ Find best matching supervised topic for each LDA topic\n",
    "best_matches = {}\n",
    "\n",
    "for lda_id, lda_words in lda_top_words.items():\n",
    "    lda_set = set(lda_words)\n",
    "    best_topic = None\n",
    "    best_pct = 0.0\n",
    "\n",
    "    for s_name, s_set in supervised_top_words.items():\n",
    "        overlap_pct = len(lda_set & s_set) / len(s_set) * 100\n",
    "        if overlap_pct > best_pct:\n",
    "            best_pct = overlap_pct\n",
    "            best_topic = s_name\n",
    "    best_matches[lda_id] = (best_topic, best_pct)\n",
    "\n",
    "total_match = 0\n",
    "print(\"LDA topic -> Best matching Clunky Method topic (% overlap):\\n\")\n",
    "for lda_id, (topic_name, pct) in best_matches.items():\n",
    "    print(f\"LDA topic {lda_id}: {topic_name} ({pct:.1f}% match)\")\n",
    "    total_match += pct\n",
    "overall_matching = total_match/10\n",
    "print(f'Overall match: {overall_matching:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff250ba4-2604-4f68-a260-ca99763be9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(reuters.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bceaeed-ee43-4522-a080-2a4fe5de034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.02707423580786\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "for doc in tokenized_docs:\n",
    "    token_count = 0\n",
    "    for word, t_count in doc.items():\n",
    "        token_count += t_count\n",
    "    counts.append(token_count)\n",
    "print(sum(counts)/len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0591d14-9aa9-452d-ab1a-4861028e0ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9160\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57cab2c5-b211-4e21-a7dc-3752b3986d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "topic_list = []\n",
    "for doc in docs:\n",
    "    topic_list.extend(docs[doc]['topics'])\n",
    "unique_topics = list(set(topic_list))\n",
    "print(len(unique_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458415a-807e-4341-ab23-69b6ce4567ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
